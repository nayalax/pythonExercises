{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrapy.db import AstraDB\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "token = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
    "dbEndpoint = os.getenv(\"ASTRA_DB_API_ENDPOINT\")\n",
    "openaiToken = os.getenv(\"OPENAI_API_KEY\")\n",
    "csvPath = (\"../project/data/rotten_tomatoes_movies.csv\")\n",
    "collectionName = \"vector_movies\"\n",
    "cleanedCsvPath = (\"../project/data/moviesDataSetCleaned.csv\")\n",
    "\n",
    "# Configurar el motor de OpenAI\n",
    "engine = \"gpt-4\"\n",
    "embeddings = OpenAIEmbeddings(api_key=openaiToken, model=\"text-embedding-3-large\")\n",
    "\n",
    "# def get_embeddings(text):\n",
    "#     query_results = embeddings.embed_query(text)\n",
    "#     print(query_results)\n",
    "#     return query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataset before we try sending it to AstraDB\n",
    "columns = ['movie_title', 'genres', 'directors', 'tomatometer_rating']\n",
    "df = pd.read_csv(csvPath)\n",
    "df = df[columns]\n",
    "df.columns = [column.strip() for column in df.columns]\n",
    "df.dropna(axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# dfCombined = pd.DataFrame(columns=['movies'])\n",
    "df['summary'] = df['movie_title'].astype(str) + \": \" + df['directors'].astype(str) + \": \" + df['tomatometer_rating'].astype(str)\n",
    "# dfCombined.to_csv(cleanedCsvPath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inserted 15 documents.\n",
      "im at index: 14\n",
      "\n",
      "Inserted 15 documents.\n",
      "im at index: 29\n",
      "\n",
      "Inserted 15 documents.\n",
      "im at index: 44\n",
      "\n",
      "Inserted 15 documents.\n",
      "im at index: 59\n",
      "\n",
      "Inserted 15 documents.\n",
      "im at index: 74\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m documents\u001b[38;5;241m.\u001b[39mappend(document)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(documents) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m15\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m     inserted_ids \u001b[38;5;241m=\u001b[39m \u001b[43mvstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInserted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inserted_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m     documents \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\langchain_core\\vectorstores.py:119\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    118\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\langchain_astradb\\vectorstores\\astradb.py:625\u001b[0m, in \u001b[0;36mAstraDBVectorStore.add_texts\u001b[1;34m(self, texts, metadatas, ids, batch_size, batch_concurrency, overwrite_concurrency, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch_inserted \u001b[38;5;241m+\u001b[39m batch_replaced\n\u001b[0;32m    624\u001b[0m _b_max_workers \u001b[38;5;241m=\u001b[39m batch_concurrency \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbulk_insert_batch_concurrency\n\u001b[1;32m--> 625\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_b_max_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtpe\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_ids_nested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_handle_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_iterate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [iid \u001b[38;5;28;01mfor\u001b[39;00m id_list \u001b[38;5;129;01min\u001b[39;00m all_ids_nested \u001b[38;5;28;01mfor\u001b[39;00m iid \u001b[38;5;129;01min\u001b[39;00m id_list]\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\concurrent\\futures\\_base.py:647\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[1;32m--> 647\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\concurrent\\futures\\thread.py:238\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[1;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[1;32m--> 238\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\threading.py:1147\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\threading.py:1167\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1168\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "# db = AstraDB(\n",
    "#   token=token,\n",
    "#   api_endpoint=dbEndpoint)\n",
    "# # Create a collection\n",
    "# collection = db.create_collection(collectionName, dimension=3072, metric=\"cosine\")\n",
    "\n",
    "# Initialize Langchain vector store\n",
    "vstore = AstraDBVectorStore (\n",
    "    embedding=embeddings,\n",
    "    collection_name=collectionName,\n",
    "    token=token,\n",
    "    api_endpoint=dbEndpoint\n",
    ")\n",
    "# print (f\"Connected to AstraDB: {db.get_collections()}\")\n",
    "\n",
    "# Procesar cada línea y obtener los embeddings\n",
    "# Leer el documento de texto\n",
    "documents = []\n",
    "\n",
    "for index, row in enumerate(df['summary']):\n",
    "    metadata = {\"genres\": df['genres'][index]}\n",
    "    # print(f\"INDEX: {i} \\n We are in row {row} and the genres are: \\n {df['genres'][index]}\")\n",
    "    document = Document(page_content=row, metadata=metadata)\n",
    "    documents.append(document)\n",
    "    if len(documents) == 15:\n",
    "        inserted_ids = vstore.add_documents(documents)\n",
    "        print(f\"\\nInserted {len(inserted_ids)} documents.\")\n",
    "        documents = []\n",
    "        inserted_ids = []\n",
    "        print(f\"im at index: {index}\")\n",
    "# with open(cleanedCsvPath, 'r', encoding=\"utf8\") as file:\n",
    "#     lines = file.readlines()\n",
    "    \n",
    "# if os.path.exists('documents.json'):\n",
    "#     with open('documents.json', 'r') as file:\n",
    "#         documents = json.load(file)\n",
    "#     for document in documents:\n",
    "#         res = collection.upsert_one(document)\n",
    "# else:\n",
    "#     for index, line in enumerate(lines):\n",
    "#         # Extraer el texto de cada línea\n",
    "#         # Obtener el embedding para el texto\n",
    "#         vector = get_embeddings(line)\n",
    "#         # Crear el documento\n",
    "#         document = {\n",
    "#             \"_id\": str(index + 1),\n",
    "#             \"text\": line,\n",
    "#             \"vector\": vector\n",
    "#         }\n",
    "#         # Insertar el documento en la base de datos\n",
    "#         # Reemplaza 'collection' con tu objeto de colección de la base de datos \n",
    "#         # res = vstore.add_documents(document)  # Aqui usamos upsert si existe se actualiza si no, se crea # This errors out with 'str' object has no attribute 'page_content'\n",
    "#         res = collection.upsert_one(document)\n",
    "#         documents.append(document)\n",
    "        \n",
    "#         # Guardar los documentos en un archivo JSON\n",
    "#         with open ('documents.json', 'w') as file:\n",
    "#             json.dump(documents, file)\n",
    "#         print (documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
